{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V 基于TensorFlow的基础实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 准备阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaolin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Give a folder path as an argument with '--log_dir' to save\n",
    "# TensorBoard summaries. Default is a log folder in current directory.\n",
    "\n",
    "current_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--log_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'log'),\n",
    "    help='The log directory for TensorBoard summaries.')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Create the directory for TensorBoard variables if there is not.\n",
    "if not os.path.exists(FLAGS.log_dir):\n",
    "  os.makedirs(FLAGS.log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Download the data.\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "\n",
    "# pylint: disable=redefined-outer-name\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename,\n",
    "                                                       local_filename)\n",
    "    statinfo = os.stat(local_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + local_filename +\n",
    "                        '. Can you get to it with a browser?')\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "# filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        # 读取 数据后返回 Unicode 的字符串\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "# vocabulary = read_data(filename)\n",
    "# vocabulary 返回的是一个数组，里面是单个的分好的词\n",
    "#print('Data size', len(vocabulary))\n",
    "#print (\"Data Samples\",vocabulary[:20])\n",
    "\n",
    "vocabulary = [\n",
    "    'anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first',\n",
    "    'used', 'against', 'early', 'working', 'class', 'radicals', 'including',\n",
    "    'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the',\n",
    "    'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the',\n",
    "    'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to',\n",
    "    'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to',\n",
    "    'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also',\n",
    "    'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self',\n",
    "    'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived',\n",
    "    'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king',\n",
    "    'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief',\n",
    "    'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished',\n",
    "    'although', 'there', 'are', 'differing', 'interpretations', 'of', 'what',\n",
    "    'this', 'means', 'anarchism', 'also', 'refers', 'to', 'related', 'social',\n",
    "    'movements', 'that', 'advocate', 'the', 'elimination', 'of',\n",
    "    'authoritarian', 'institutions', 'particularly', 'the', 'state', 'the',\n",
    "    'word', 'anarchy', 'as', 'most', 'anarchists', 'use', 'it', 'does', 'not',\n",
    "    'imply', 'chaos', 'nihilism', 'or', 'anomie', 'but', 'rather', 'a',\n",
    "    'harmonious', 'anti', 'authoritarian', 'society', 'in', 'place', 'of',\n",
    "    'what', 'are', 'regarded', 'as', 'authoritarian', 'political',\n",
    "    'structures', 'and', 'coercive', 'economic', 'institutions', 'anarchists',\n",
    "    'advocate', 'social', 'relations', 'based', 'upon', 'voluntary',\n",
    "    'association', 'of', 'autonomous', 'individuals', 'mutual', 'aid', 'and',\n",
    "    'self', 'governance', 'while', 'anarchism', 'is', 'most', 'easily',\n",
    "    'defined', 'by', 'what', 'it', 'is', 'against', 'anarchists', 'also',\n",
    "    'offer', 'positive', 'visions', 'of', 'what', 'they', 'believe', 'to',\n",
    "    'be', 'a', 'truly', 'free', 'society'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建的常用词的词典： \n",
      " {'UNK': 0, 'the': 1, 'of': 2, 'a': 3, 'anarchism': 4, 'as': 5, 'is': 6, 'and': 7, 'to': 8, 'anarchists': 9, 'what': 10, 'used': 11, 'that': 12, 'society': 13, 'it': 14, 'also': 15, 'are': 16, 'authoritarian': 17, 'term': 18, 'against': 19, 'revolution': 20, 'in': 21, 'means': 22, 'positive': 23, 'by': 24, 'self': 25, 'defined': 26, 'word': 27, 'political': 28, 'be': 29, 'social': 30, 'advocate': 31, 'institutions': 32, 'most': 33, 'originated': 34, 'abuse': 35, 'first': 36, 'early': 37, 'working': 38, 'class': 39, 'radicals': 40, 'including': 41, 'diggers': 42, 'english': 43, 'sans': 44, 'culottes': 45, 'french': 46, 'whilst': 47, 'still': 48, 'pejorative': 49}\n",
      "Sample count [['UNK', 73], ('the', 12), ('of', 9), ('a', 6), ('anarchism', 5)]\n",
      "Most common words (+UNK) [['UNK', 73], ('the', 12), ('of', 9), ('a', 6), ('anarchism', 5)]\n",
      "Sample data [4, 34, 5, 3, 18, 2, 35, 36, 11, 19] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 50\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    \n",
    "    # collections.Counter 统计词频，most_common 返回TOP N 的词\n",
    "    # 在上面的 count 数组后面追加，最后 count 是一个包含了所有词的词频的列表\n",
    "    \n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for word, _ in count:\n",
    "        \n",
    "        # 添加字典，给每一次词添加一个编号\n",
    "        # 最后构建的词典是每一个词，对应其编号\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    print(\"构建的常用词的词典：\",\"\\n\", dictionary)\n",
    "    \n",
    "    data = list()\n",
    "    \n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        \n",
    "        # 得到每一个词对应的编号,如果不在上面的词典中那么就不是高频词，对应的词典就是UNK\n",
    "        \n",
    "        index = dictionary.get(word, 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        \n",
    "        # 得到输入的分好词的 数据的对应的编号数据\n",
    "        data.append(index)\n",
    "    \n",
    "    count[0][1] = unk_count # 把最新统计出来的不是高频词的编号给第一个UNK\n",
    "    \n",
    "    # 翻转 上面的 键值对\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "# Filling 4 global variables:\n",
    "# data - list of codes (integers from 0 to vocabulary_size-1).\n",
    "#   This is the original text but words are replaced by their codes\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# dictionary - map of words(strings) to their codes(integers)\n",
    "# reverse_dictionary - maps codes(integers) to words(strings)\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(\n",
    "    vocabulary, vocabulary_size)\n",
    "\n",
    "#del vocabulary  # 这里是为了降低内存的占用，如果是小数据量的可以去除\n",
    "print('Sample count', count[:5])\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 originated -> 5 as\n",
      "34 originated -> 4 anarchism\n",
      "5 as -> 34 originated\n",
      "5 as -> 3 a\n",
      "3 a -> 18 term\n",
      "3 a -> 5 as\n",
      "18 term -> 2 of\n",
      "18 term -> 3 a\n"
     ]
    }
   ],
   "source": [
    "# 下面采用的是skip-gram 的方法\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    \n",
    "    global data_index  # 声明使用全局变量\n",
    "    \n",
    "    assert batch_size % num_skips == 0  #做类似于 JAVA TRY CATCH 的判断，断言前面两个变量的关系，没有余数\n",
    "    \n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    # 声明两个变量，一个是 batch 大小的数组，用来存样本，一个是标签，只是纵向的数组\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    # 创建一个两端开口的管道\n",
    "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span  # 滑动窗口\n",
    "\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)  # 随机采样\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1) #num_skip 表示对应的目标数\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "          reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 34  5  5  3  3 18 18]\n",
      "[[ 5]\n",
      " [ 4]\n",
      " [34]\n",
      " [ 3]\n",
      " [18]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 3]]\n"
     ]
    }
   ],
   "source": [
    "print (batch)\n",
    "print (labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-a9531b247fe6>:78: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "normalized_embeddings: Tensor(\"truediv:0\", shape=(50, 128), dtype=float32)\n",
      "valid_embeddings: Tensor(\"embedding_lookup:0\", shape=(16, 128), dtype=float32)\n",
      "similarity: Tensor(\"MatMul:0\", shape=(16, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "\n",
    "            # 向量的初始化，生成词数和向量维度的随机分布的矩阵\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0,\n",
    "                                  1.0))\n",
    "\n",
    "            # 找到对应输入 batch 的词的索引对应的初始化的向量\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)#真正最后的向量就是使用类似的方法获取\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            # 初始化权重矩阵，权重都是正态的分布的\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal(\n",
    "                    [vocabulary_size, embedding_size],\n",
    "                    stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "\n",
    "# 下面是损失函数，其中 NCE 是 W2V 专用的多分类损失函数\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    # 显示损失函数的下降过程\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    # 逆向传播使用的 SGD\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    # 做类似归一化处理\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True)) #计算分母的值\n",
    "    normalized_embeddings = embeddings / norm #归一化处理\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                              valid_dataset)\n",
    "    \n",
    "    print(\"normalized_embeddings:\",normalized_embeddings)\n",
    "    print(\"valid_embeddings:\",valid_embeddings)\n",
    "    \n",
    "    # 矩阵相乘\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    print(\"similarity:\",similarity)\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    # 最后取的是权重矩阵中的对应的那个词的权重\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "\n",
    "        # 生成 batch 形式的样本\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                                    skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        # 可视化 summary\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "            [optimizer, merged, loss],\n",
    "            feed_dict=feed_dict,\n",
    "            run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            \n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(FLAGS.log_dir + '/metadata.tsv', 'w') as f:\n",
    "        for i in xrange(vocabulary_size):\n",
    "            f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(FLAGS.log_dir, 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "\n",
    "        # 生成 batch 形式的样本\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                                    skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        # 可视化 summary\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "            [optimizer, merged, loss],\n",
    "            feed_dict=feed_dict,\n",
    "            run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            \n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(FLAGS.log_dir + '/metadata.tsv', 'w') as f:\n",
    "        for i in xrange(vocabulary_size):\n",
    "            f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(FLAGS.log_dir, 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
